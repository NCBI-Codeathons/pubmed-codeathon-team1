<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2019//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_190101.dtd">
<PubmedArticleSet>
  <PubmedArticle>
    <MedlineCitation Status="MEDLINE" Owner="NLM" IndexingMethod="Automated">
      <PMID Version="1">35310080</PMID>
      <DateCompleted>
        <Year>2022</Year>
        <Month>04</Month>
        <Day>22</Day>
      </DateCompleted>
      <DateRevised>
        <Year>2022</Year>
        <Month>04</Month>
        <Day>25</Day>
      </DateRevised>
      <Article PubModel="Electronic-eCollection">
        <Journal>
          <ISSN IssnType="Electronic">2666-1667</ISSN>
          <JournalIssue CitedMedium="Internet">
            <Volume>3</Volume>
            <Issue>2</Issue>
            <PubDate>
              <Year>2022</Year>
              <Month>Jun</Month>
              <Day>17</Day>
            </PubDate>
          </JournalIssue>
          <Title>STAR protocols</Title>
          <ISOAbbreviation>STAR Protoc</ISOAbbreviation>
        </Journal>
        <ArticleTitle>Speech-to-Speech Synchronization protocol to classify human participants as high or low auditory-motor synchronizers.</ArticleTitle>
        <Pagination>
          <StartPage>101248</StartPage>
          <MedlinePgn>101248</MedlinePgn>
        </Pagination>
        <ELocationID EIdType="pii" ValidYN="Y">101248</ELocationID>
        <ELocationID EIdType="doi" ValidYN="Y">10.1016/j.xpro.2022.101248</ELocationID>
        <Abstract>
          <AbstractText>The ability to synchronize a motor action to a rhythmic auditory stimulus is often considered an innate human skill. However, some individuals lack the ability to synchronize speech to a perceived syllabic rate. Here, we describe a simple and fast protocol to classify a single native English speaker as being or not being a speech synchronizer. This protocol consists of four parts: the pretest instructions and volume adjustment, the training procedure, the execution of the main task, and data analysis. For complete details on the use and execution of this protocol, please refer to Assaneo et al. (2019a).</AbstractText>
          <CopyrightInformation>© 2022 The Author(s).</CopyrightInformation>
        </Abstract>
        <AuthorList CompleteYN="Y">
          <Author ValidYN="Y">
            <LastName>Lizcano-Cortés</LastName>
            <ForeName>Fernando</ForeName>
            <Initials>F</Initials>
            <AffiliationInfo>
              <Affiliation>Institute of Neurobiology, UNAM, Querétaro 76230, México.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Gómez-Varela</LastName>
            <ForeName>Ireri</ForeName>
            <Initials>I</Initials>
            <AffiliationInfo>
              <Affiliation>Institute of Neurobiology, UNAM, Querétaro 76230, México.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Mares</LastName>
            <ForeName>Cecilia</ForeName>
            <Initials>C</Initials>
            <AffiliationInfo>
              <Affiliation>Institute of Neurobiology, UNAM, Querétaro 76230, México.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Wallisch</LastName>
            <ForeName>Pascal</ForeName>
            <Initials>P</Initials>
            <AffiliationInfo>
              <Affiliation>Department of Psychology, New York University, New York, NY 10003, USA.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Orpella</LastName>
            <ForeName>Joan</ForeName>
            <Initials>J</Initials>
            <AffiliationInfo>
              <Affiliation>Department of Psychology, New York University, New York, NY 10003, USA.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Poeppel</LastName>
            <ForeName>David</ForeName>
            <Initials>D</Initials>
            <AffiliationInfo>
              <Affiliation>Department of Psychology, New York University, New York, NY 10003, USA.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Ernst Struengmann Institute for Neuroscience, 60528 Frankfurt, Germany.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Center for Language, Music and Emotion (CLaME), New York University, New York, NY, USA.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Max Plank Institute for Empirical Aesthetics, 60322 Frankfurt, Germany.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Ripollés</LastName>
            <ForeName>Pablo</ForeName>
            <Initials>P</Initials>
            <AffiliationInfo>
              <Affiliation>Department of Psychology, New York University, New York, NY 10003, USA.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Center for Language, Music and Emotion (CLaME), New York University, New York, NY, USA.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Max Plank Institute for Empirical Aesthetics, 60322 Frankfurt, Germany.</Affiliation>
            </AffiliationInfo>
            <AffiliationInfo>
              <Affiliation>Music and Audio Research Laboratory (MARL), New York University, New York, NY 11201, USA.</Affiliation>
            </AffiliationInfo>
          </Author>
          <Author ValidYN="Y">
            <LastName>Assaneo</LastName>
            <ForeName>M Florencia</ForeName>
            <Initials>MF</Initials>
            <AffiliationInfo>
              <Affiliation>Institute of Neurobiology, UNAM, Querétaro 76230, México.</Affiliation>
            </AffiliationInfo>
          </Author>
        </AuthorList>
        <Language>eng</Language>
        <PublicationTypeList>
          <PublicationType UI="D016428">Journal Article</PublicationType>
          <PublicationType UI="D013485">Research Support, Non-U.S. Gov't</PublicationType>
        </PublicationTypeList>
        <ArticleDate DateType="Electronic">
          <Year>2022</Year>
          <Month>03</Month>
          <Day>15</Day>
        </ArticleDate>
      </Article>
      <MedlineJournalInfo>
        <MedlineTA>STAR Protoc</MedlineTA>
        <NlmUniqueID>101769501</NlmUniqueID>
        <ISSNLinking>2666-1667</ISSNLinking>
      </MedlineJournalInfo>
      <CitationSubset>IM</CitationSubset>
      <MeshHeadingList>
        <MeshHeading>
          <DescriptorName UI="D000161" MajorTopicYN="Y">Acoustic Stimulation</DescriptorName>
        </MeshHeading>
        <MeshHeading>
          <DescriptorName UI="D006801" MajorTopicYN="N">Humans</DescriptorName>
        </MeshHeading>
        <MeshHeading>
          <DescriptorName UI="D013060" MajorTopicYN="Y">Speech</DescriptorName>
        </MeshHeading>
      </MeshHeadingList>
      <KeywordList Owner="NOTNLM">
        <Keyword MajorTopicYN="Y">Behavior</Keyword>
        <Keyword MajorTopicYN="Y">Clinical Protocol</Keyword>
        <Keyword MajorTopicYN="Y">Cognitive Neuroscience</Keyword>
        <Keyword MajorTopicYN="Y">Neuroscience</Keyword>
      </KeywordList>
      <CoiStatement>The authors declare no competing interests.</CoiStatement>
    </MedlineCitation>
    <PubmedData>
      <History>
        <PubMedPubDate PubStatus="entrez">
          <Year>2022</Year>
          <Month>3</Month>
          <Day>21</Day>
          <Hour>8</Hour>
          <Minute>58</Minute>
        </PubMedPubDate>
        <PubMedPubDate PubStatus="pubmed">
          <Year>2022</Year>
          <Month>3</Month>
          <Day>22</Day>
          <Hour>6</Hour>
          <Minute>0</Minute>
        </PubMedPubDate>
        <PubMedPubDate PubStatus="medline">
          <Year>2022</Year>
          <Month>4</Month>
          <Day>23</Day>
          <Hour>6</Hour>
          <Minute>0</Minute>
        </PubMedPubDate>
      </History>
      <PublicationStatus>epublish</PublicationStatus>
      <ArticleIdList>
        <ArticleId IdType="pubmed">35310080</ArticleId>
        <ArticleId IdType="pmc">PMC8931471</ArticleId>
        <ArticleId IdType="doi">10.1016/j.xpro.2022.101248</ArticleId>
        <ArticleId IdType="pii">S2666-1667(22)00128-9</ArticleId>
      </ArticleIdList>
      <ReferenceList>
        <Reference>
          <Citation>Assaneo M.F., Ripollés P., Orpella J., Lin W.M., de Diego-Balaguer R., Poeppel D. Spontaneous synchronization to speech reveals neural mechanisms facilitating language learning. Nat. Neurosci. 2019;22:627–632.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC6435400</ArticleId>
            <ArticleId IdType="pubmed">30833700</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Assaneo M.F., Rimmele J.M., Orpella J., Ripollés P., de Diego-Balaguer R., Poeppel D. The lateralization of speech-brain coupling is differentially modulated by intrinsic auditory and top-down mechanisms. Front. Integr. Neurosci. 2019;13:28.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC6650591</ArticleId>
            <ArticleId IdType="pubmed">31379527</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Anwyl-Irvine A.L., Massonnié J., Flitton A., Kirkham N., Evershed J.K. Gorilla in our midst: An online behavioral experiment builder. Behav. Res. 2020;52:388–407.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC7005094</ArticleId>
            <ArticleId IdType="pubmed">31016684</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Assaneo M.F., Orpella J., Ripollés P., Noejovich L., López-Barroso D., Diego-Balaguer R. de, Poeppel D. Population-level differences in the neural substrates supporting statistical learning. bioRxiv. 2020 Preprint at. 2020.07.03.187260.</Citation>
        </Reference>
        <Reference>
          <Citation>Assaneo M.F., Rimmele J.M., Sanz Perl Y., Poeppel D. Speaking rhythmically can shape hearing. Nat. Hum. Behav. 2021;5:71–82.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pubmed">33046860</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Boersma P., Weenink D. PRAAT, a system for doing phonetics by computer. Glot Int. 2001;5:341–345.</Citation>
        </Reference>
        <Reference>
          <Citation>Brainard D.H. The Psychophysics Toolbox. Spat. Vis. 1997;10:433–436.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pubmed">9176952</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Crawford J.R., Garthwaite P.H. Investigation of the single case in neuropsychology: confidence limits on the abnormality of test scores and test score differences. Neuropsychologia. 2002;40:1196–1208.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pubmed">11931923</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Keppel G., Wickens T.D.  Pearson College Div; 2004. Design and Analysis: A Researcher’s Handbook.</Citation>
        </Reference>
        <Reference>
          <Citation>Kern P., Assaneo M.F., Endres D., Poeppel D., Rimmele J.M. Preferred auditory temporal processing regimes and auditory-motor synchronization. Psychon. Bull. Rev. 2021;28:1860–1873.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC8642338</ArticleId>
            <ArticleId IdType="pubmed">34100222</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Müllensiefen D., Gingras B., Musil J., Stewart L. The musicality of non-musicians: an index for assessing musical sophistication in the general population. PLoS ONE. 2014;9:e89642.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC3935919</ArticleId>
            <ArticleId IdType="pubmed">24586929</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>Parra-Frutos I. Testing homogeneity of variances with unequal sample sizes. Comput. Stat. 2013;28:1269–1297.</Citation>
        </Reference>
        <Reference>
          <Citation>Rusticus S., Lovato C. Impact of sample size and variability on the power and type I error rates of equivalence tests: a simulation study. Pract. Assess. Res. Eval. 2019;19:11.</Citation>
        </Reference>
        <Reference>
          <Citation>Woods K.J.P., Siegel M.H., Traer J., McDermott J.H. Headphone screening to facilitate web-based auditory experiments. Atten. Percept. Psychophys. 2017;79:2064–2072.</Citation>
          <ArticleIdList>
            <ArticleId IdType="pmc">PMC5693749</ArticleId>
            <ArticleId IdType="pubmed">28695541</ArticleId>
          </ArticleIdList>
        </Reference>
        <Reference>
          <Citation>World Health Organization  . 2015. Make Listening Safe. (No. WHO/NMH/NVI/15.2)</Citation>
        </Reference>
      </ReferenceList>
    </PubmedData>
  </PubmedArticle>
</PubmedArticleSet>
